{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "849aa228-599d-4b3c-89be-1cff236b2779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c67b2a0-77de-4efe-8b0d-adc675290c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 19:04:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff12047-5f4d-4841-9dd5-b0625fee8852",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-01.parquet #jan 2024\n",
    "!rm fhvhv_tripdata_2024-01.parquet\n",
    "df = spark.read.parquet('fhvhv_tripdata_2024-01.parquet')\n",
    "#df.show()\n",
    "df.head(3)\n",
    "df.schema\n",
    "df.printSchema()\n",
    "df.count()\n",
    "df_partitioned = df.repartition(16)\n",
    "df_partitioned.rdd.getNumPartitions()\n",
    "df_partitioned.write.parquet(\"fhv_partitioned\", mode='overwrite')\n",
    "# read the partitioned data\n",
    "df = spark.read.parquet('fhv_partitioned')\n",
    "# lazy / transformation - select, filter, groupby, joins\n",
    "df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \\\n",
    "  .filter(df.hvfhs_license_num == 'HV0003')\n",
    "# eager / actions - #show, take head, write\n",
    "df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \\\n",
    "  .filter(df.hvfhs_license_num == 'HV0003').show(3)\n",
    "\n",
    "# ^just like sql, but pyspark is more flexible\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be648667-68db-42f8-a708-115a10b7fe98",
   "metadata": {},
   "source": [
    "udf - user defined functions\n",
    "\n",
    "- huge list of functions\n",
    "- can also define out own function\n",
    "- and this is not what you would typically do in data warehouses\n",
    "bcz there defining your own fucntions is cumbersome.\n",
    "- with some complicates cases you end with ith a bunch of case statements in sql, making it difficult to test, unlike python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7519f95-f5de-4f7d-820a-7d2d99356c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "F.to_date()\n",
    "def crazy_stuff(base_num):\n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0:\n",
    "        return f's/{num:03x}'\n",
    "    elif num % 3 == 0:\n",
    "        return f'a/{num:03x}'\n",
    "    else:\n",
    "        return f'e/{num:03x}'\n",
    "\n",
    "crazy_stuff('B02884')\n",
    "# convert it to udf using udf()\n",
    "\n",
    "from pyspark.sql import types\n",
    "crazy_stuff_udf = F.udf(crazy_stuff, returnType=types.StringType())\n",
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .withColumn('base_id', crazy_stuff_udf(df.dispatching_base_num)) \\\n",
    "    .select('base_id', 'pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c7032d-b3d0-445e-81d5-d12898ed5eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_yellow = spark.read.parquet('../data/raw/yellow/*/*')\n",
    "df_yellow.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b104f6ad-b0e0-4548-97f2-190113bde9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "int_columns = [\"passenger_count\", \"RatecodeID\", \"payment_type\"]\n",
    "for col in int_columns:\n",
    "    df_yellow = df_yellow.withColumn(col, df_yellow[col].cast(IntegerType()))\n",
    "df_yellow.select('payment_type').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df5fd284-6f59-4cdc-bf03-107c91b17c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: data/pq/green/2024/01\n",
      "Processed and saved: data/pq/green/2024/02\n",
      "Processed and saved: data/pq/green/2024/03\n",
      "Processed and saved: data/pq/green/2024/04\n",
      "Processed and saved: data/pq/green/2024/05\n",
      "Processed and saved: data/pq/green/2024/06\n",
      "Processed and saved: data/pq/green/2024/07\n",
      "Processed and saved: data/pq/green/2024/08\n",
      "Processed and saved: data/pq/green/2024/09\n",
      "Processed and saved: data/pq/green/2024/10\n",
      "Processed and saved: data/pq/green/2024/11\n",
      "Processed and saved: data/pq/green/2024/12\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "TAXI_TYPE = \"green\"\n",
    "YEAR = \"2024\"\n",
    "INPUT_BASE = f\"data/raw/{TAXI_TYPE}/{YEAR}\"\n",
    "OUTPUT_BASE = f\"data/pq/{TAXI_TYPE}/{YEAR}\"\n",
    "\n",
    "# Columns to cast to IntegerType\n",
    "int_columns = [\"passenger_count\", \"RatecodeID\", \"payment_type\"]\n",
    "\n",
    "# Loop through each month\n",
    "for month in range(1, 13):\n",
    "    month_str = f\"{month:02d}\"\n",
    "    input_path = os.path.join(INPUT_BASE, month_str, f\"{TAXI_TYPE}_tripdata_{YEAR}_{month_str}.parquet\")\n",
    "    output_path = os.path.join(OUTPUT_BASE, month_str)\n",
    "\n",
    "    # Read\n",
    "    df = spark.read.parquet(input_path)\n",
    "\n",
    "    # Cast columns\n",
    "    for col in int_columns:\n",
    "        if col in df.columns:\n",
    "            df = df.withColumn(col, df[col].cast(IntegerType()))\n",
    "\n",
    "    # Write\n",
    "    df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "    print(f\"Processed and saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da113201-9133-48d3-9d1d-5c1fcc3148ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- trip_type: long (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "660218"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green = spark.read.parquet('../data/raw/green/*/*')\n",
    "df_green.printSchema()\n",
    "df_green.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe9e9f6-6f98-4d6d-90fb-ff9e7604945e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2465e42-6933-4f59-a0aa-63374f645635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4530404e-4739-44a7-ac66-8671dc0c2184",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d2ee47-e04d-4c5c-b926-dbc9a808f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = spark.read.parquet('data/pq/green/*/*')\n",
    "df_green.printSchema()\n",
    "df_yellow = spark.read.parquet('data/pq/yellow/*/*')\n",
    "df_yellow.printSchema()\n",
    "set(df_green.columns) & set(df_yellow.columns)\n",
    "df_green = df_green \\\n",
    "    .withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n",
    "    .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')\n",
    "\n",
    "df_yellow = df_yellow \\\n",
    "    .withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n",
    "    .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')\n",
    "# reorder green df column to math yellow df\n",
    "common_colums = []\n",
    "\n",
    "yellow_columns = set(df_yellow.columns)\n",
    "\n",
    "for col in df_green.columns:\n",
    "    if col in yellow_columns:\n",
    "        common_colums.append(col)\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_green_sel = df_green \\\n",
    "    .select(common_colums) \\\n",
    "    .withColumn('service_type', F.lit('green'))\n",
    "\n",
    "df_yellow_sel = df_yellow \\\n",
    "    .select(common_colums) \\\n",
    "    .withColumn('service_type', F.lit('yellow'))\n",
    "\n",
    "df_trips_data = df_green_sel.unionAll(df_yellow_sel)\n",
    "\n",
    "df_trips_data.groupBy('service_type').count().show()\n",
    "# run sql\n",
    "df_trips_data.registerTempTable('trips_data')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    service_type,\n",
    "    count(1)\n",
    "FROM\n",
    "    trips_data\n",
    "GROUP BY \n",
    "    service_type\n",
    "\"\"\").show()\n",
    "df_result = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    -- Revenue grouping \n",
    "    PULocationID AS revenue_zone,\n",
    "    date_trunc('month', pickup_datetime) AS revenue_month, \n",
    "    service_type, \n",
    "\n",
    "    -- Revenue calculation \n",
    "    SUM(fare_amount) AS revenue_monthly_fare,\n",
    "    SUM(extra) AS revenue_monthly_extra,\n",
    "    SUM(mta_tax) AS revenue_monthly_mta_tax,\n",
    "    SUM(tip_amount) AS revenue_monthly_tip_amount,\n",
    "    SUM(tolls_amount) AS revenue_monthly_tolls_amount,\n",
    "    SUM(improvement_surcharge) AS revenue_monthly_improvement_surcharge,\n",
    "    SUM(total_amount) AS revenue_monthly_total_amount,\n",
    "    SUM(congestion_surcharge) AS revenue_monthly_congestion_surcharge,\n",
    "\n",
    "    -- Additional calculations\n",
    "    AVG(passenger_count) AS avg_monthly_passenger_count,\n",
    "    AVG(trip_distance) AS avg_monthly_trip_distance\n",
    "FROM\n",
    "    trips_data\n",
    "GROUP BY\n",
    "    1, 2, 3\n",
    "\"\"\")\n",
    "\n",
    "df_result.show()\n",
    "df_result.coalesce(1).write.parquet('data/report/', mode='overwrite')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41fd20dc-1fa5-4ac6-b5b0-11333d87c517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 19:33:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://de-vm.asia-south1-c.c.velvety-tangent-463717-h8.internal:7077\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "# spark://de-vm.asia-south1-c.c.velvety-tangent-463717-h8.internal:7077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71bfc361-f7dd-40ab-ace5-96a6910664c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/25 19:35:59 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/06/25 19:36:14 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/06/25 19:36:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/06/25 19:36:44 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_green = spark.read.parquet('data/pq/green/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c673d-dc19-4241-ba16-7ae841548df0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b3bf08-5530-4597-8c84-e5cfa84566c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19611b7e-ab73-42f2-b248-dcd39df7c87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head taxi_zone_lookup.csv\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi_zone_lookup.csv')\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi_zone_lookup.csv')\n",
    "\n",
    "df.show()\n",
    "df.write.parquet('zones')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
